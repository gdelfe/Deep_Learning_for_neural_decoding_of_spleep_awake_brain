{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Compared with baseline (separate classes, specified val/test dates, 1CH, Goose 1st):\n",
    "* Try combining classes (CNN_Attention_Combined.ipynb)\n",
    "    * Roughly the same accuracy\n",
    "    * No impact on val loss lower than train loss\n",
    "* Try mixing data from different dates (CNN_Attention_Mixed_Dates.ipynb)\n",
    "    * Train loss similar, val loss lower\n",
    "    * Train loss lower than val loss\n",
    "* Try 62 CHs (CNN_Attention_CH62.ipynb)\n",
    "    * Unfinished\n",
    "    * Use Gino's best CNN 62CH model\n",
    "    * Modify input dimensions of last linear layer of CNN if needed\n",
    "* Train on Goose and predict on Jester (CNN_Attention_Goose_Jester.ipynb)\n",
    "    * Existence of peaks of val accuracies \n",
    "    * Without tuning, around 0.82 val accuracy\n",
    "    * By tuning attention dimension and/or T_length, could reach 0.90 val accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "import operator\n",
    "import pdb\n",
    "import random\n",
    "from random import sample\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define \n",
    "channel, width of each spectrogram in the time direction and bad dates (with artifacts), validation dates, test dates, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH = 23\n",
    "time_window = 10 # width of the spectrogram in the time direction \n",
    "T_length = 3 # how many spectrograms grouped together\n",
    "proceed = 1 # Parameter controlling overlap across spectrogram. 1 = max overalp, T_length-1 = no overlap\n",
    "device = torch.device(\"cuda\")\n",
    "bad_dates = ['180326', '180328', '171019', '180715', '180716', '180717']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dates = ['180327','180329']\n",
    "test_dates = ['180330','180331']\n",
    "load_path = '/home/bijanadmin/Desktop/Goose_data/data_Goose_1st_2/'\n",
    "# save_path = '/mnt/pesaranlab/People/Capstone_students/Yue/model/...' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180324_003_62_time1036.0_sleep.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_files = os.listdir(load_path+'sleep/')\n",
    "move_files = os.listdir(load_path+'move/')\n",
    "all_files = sleep_files+move_files\n",
    "f = all_files[0]\n",
    "print(f)\n",
    "f.split('_')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the data set and make it into grouped spectrograms (windows)\n",
    "The function assigns the label 1 to sleep and label 0 to movement, creates a dictionary to store the data set and groups the data into sets of spectrograms by calling the function create_files_new_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_new(load_path, bad_dates, T_length=3, proceed=1):\n",
    "    \n",
    "    sleep_files = os.listdir(load_path+'sleep/')\n",
    "    move_files = os.listdir(load_path+'move/')\n",
    "    all_files = sleep_files+move_files\n",
    "    \n",
    "    cnt = 0\n",
    "    dic = {}\n",
    "    for f in all_files:\n",
    "        mvmt_type = f.split('_')[-1].split('.')[0]\n",
    "        date = f.split('_')[0]\n",
    "        rec = f.split('_')[1].split('_')[0]\n",
    "        time = float(f.split('_')[3][4:])\n",
    "        if date in bad_dates:\n",
    "            continue\n",
    "        if mvmt_type == 'sleep':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        if date in dic:\n",
    "            if rec in dic[date]:\n",
    "                dic[date][rec].append([f, label, mvmt_type, date, rec, time])\n",
    "            else:\n",
    "                dic[date][rec] = [[f, label, mvmt_type, date, rec, time]]\n",
    "        else:\n",
    "            dic[date] = {rec: [[f, label, mvmt_type, date, rec, time]]}\n",
    "      \n",
    "    for d in dic:\n",
    "        for r in dic[d]:\n",
    "            dic[d][r] = sorted(dic[d][r], key=operator.itemgetter(3, 4, 5)) # sort based on date, rec, ascending time interval\n",
    "            cnt += len(dic[d][r])            \n",
    "            \n",
    "    move_data, sleep_data = [], []\n",
    "    for d in dic:\n",
    "        for r in dic[d]:\n",
    "            sleep_grouped, move_grouped = create_files_new_helper(dic[d][r], T_length=T_length, proceed=proceed)\n",
    "            sleep_data.append(sleep_grouped)\n",
    "            move_data.append(move_grouped)\n",
    "    \n",
    "    return move_data, sleep_data, cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to group  a set of spectrogram windows together\n",
    "T_length = number of spectrograms grouped together <br>\n",
    "time_window = width of each spectrogram in the time direction <br>\n",
    "proceed =  number of time overlaps across different windows. <br> Proceed = 1, max overlap. Proceed = time_window-1, no overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_new_helper(L, T_length, proceed):\n",
    "    \n",
    "    # for each date and for each recording in that date \n",
    "    L_labels = np.array([L[i][1] for i in range(len(L))]) # get the labels for that specific recording\n",
    "    L_times = np.array([L[i][-1] for i in range(len(L))]) # get the starting time of that time window, e.g. 20 sec, 30 sec\n",
    "    \n",
    "    L_new_sleep, L_new_move = [], []\n",
    "    start = 0\n",
    "    while start <= len(L)-T_length:\n",
    "        end = start + T_length\n",
    "        \n",
    "        if sum(L_times[start+1:end]-L_times[start:end-1]-time_window) != 0:\n",
    "            start += 1\n",
    "            continue\n",
    "        if sum(L_labels[start:end]) == T_length:\n",
    "            L_new_sleep.append(L[start:end])\n",
    "        elif sum(L_labels[start:end]) == 0:\n",
    "            L_new_move.append(L[start:end])\n",
    "        start += proceed\n",
    "#         pdb.set_trace()\n",
    "\n",
    "    return L_new_sleep, L_new_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "move_files, sleep_files, cnt = create_files_new(load_path, bad_dates, T_length=T_length, proceed=proceed)\n",
    "train_files, val_files, test_files = [], [], []\n",
    "\n",
    "cnt_i = 0\n",
    "# split data into train, validation, test\n",
    "for f in move_files+sleep_files:\n",
    "    if f:\n",
    "        if f[0][1][3] in val_dates: #f[0][1][3] correspond to the date field\n",
    "            val_files.extend(f)\n",
    "        elif f[0][1][3] in test_dates:\n",
    "            test_files.extend(f)\n",
    "        else:\n",
    "            train_files.extend(f)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shuffle the data across groups\n",
    "random.shuffle(train_files)\n",
    "random.shuffle(val_files)\n",
    "random.shuffle(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202440"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_files)*10 + len(val_files)*10 + len(test_files)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample smaller TRAINING data set (movement) in order to have a balanced sleep/movement data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(train_files):\n",
    "    train_sleep = [i for i in train_files if i[0][1] == 1] # list of 1s\n",
    "    train_move = [i for i in train_files if i[0][1] == 0]  # list of 0s\n",
    "    diff = abs(len(train_sleep)-len(train_move))\n",
    "    train_new = []\n",
    "    d = 0\n",
    "    while d < diff:\n",
    "        if len(train_sleep) > len(train_move):\n",
    "            ind = random.randint(0, len(train_move)-1)\n",
    "            x = train_move[ind]\n",
    "            d += 1\n",
    "        else:\n",
    "            ind = random.randint(0, len(train_sleep)-1)\n",
    "            x = train_sleep[ind]\n",
    "            d += 1\n",
    "        train_new.append(x)   \n",
    "    train_files = train_sleep+train_move+train_new\n",
    "    return train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = upsample(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17266"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Balance val \n",
    "sleep_files = []\n",
    "move_files = []\n",
    "for t in range(len(val_files)):\n",
    "    if 'sleep' in val_files[t][0]:\n",
    "        sleep_files.append(val_files[t])\n",
    "    if 'move' in val_files[t][0]:\n",
    "        move_files.append(val_files[t])\n",
    "\n",
    "sleep_sample = sample(sleep_files, len(move_files))\n",
    "val_2_files = sleep_sample+move_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data volume:  17266\n",
      "validation data volume:  2004\n",
      "rate val/train 0.12\n"
     ]
    }
   ],
   "source": [
    "print('training data volume: ',len(train_files))\n",
    "print('validation data volume: ',len(val_2_files))\n",
    "print('rate val/train {:.2f}'.format(len(val_2_files)/len(train_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  print some stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17266\n",
      "[['180325_006_62_time4735.0_sleep.npy', 1, 'sleep', '180325', '006', 4735.0], ['180325_006_62_time4745.0_sleep.npy', 1, 'sleep', '180325', '006', 4745.0], ['180325_006_62_time4755.0_sleep.npy', 1, 'sleep', '180325', '006', 4755.0]]\n",
      "torch.Size([62, 10, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 62, 100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(train_files)\n",
    "print(len(train_files)) # total number of grouped spectrograms in training set \n",
    "group = train_files[2]\n",
    "f, label, mvmt_type, _, _, time = group[0]\n",
    "print(group)\n",
    "spec = torch.from_numpy(np.load(load_path+mvmt_type+'/'+f))\n",
    "print(spec.shape)\n",
    "torch.transpose(spec[:,:,:].unsqueeze(0),2,1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Set - Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDatasetAtt(Dataset):\n",
    "    def __init__(self, files, load_path, T_length, all_label=False, CH=None):\n",
    "        self.CH = CH\n",
    "        self.files = files\n",
    "        self.load_path = load_path\n",
    "        self.T_length = T_length\n",
    "        self.all_label = all_label\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        group = self.files[idx]\n",
    "        specs, labels, dates, recs, times = [], [], [], [], []\n",
    "        \n",
    "        for i in range(len(group)): # for each window in the group \n",
    "            f, label, mvmt_type, date, rec, time = group[i]\n",
    "            spec = torch.from_numpy(np.load(self.load_path+mvmt_type+'/'+f)) # load the spectrogram file\n",
    "            if self.CH is not None: \n",
    "                # get spectrogram for specific channel\n",
    "                spec = torch.transpose(spec[self.CH,:,:].unsqueeze(0), 2, 1) # careful with the dimensions order here \n",
    "            else:\n",
    "                # get spectrograms for 62 channels \n",
    "                spec = torch.transpose(spec, 2, 1) # careful with the dimension order here \n",
    "            # append spectrograms and metadata\n",
    "            specs.append(spec)\n",
    "            labels.append(torch.Tensor([label]))\n",
    "            dates.append(date)\n",
    "            recs.append(rec)\n",
    "            times.append(time)\n",
    "            if i == (self.T_length-1)/2: # what happens if T_length is even?\n",
    "                label_mid = torch.Tensor([label])\n",
    "                date_mid = date\n",
    "                rec_mid = rec\n",
    "                time_mid = time\n",
    "        if self.all_label:\n",
    "            return specs, labels, dates, recs, times\n",
    "        else:\n",
    "            return specs, label_mid, date_mid, rec_mid, time_mid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpectrogramDatasetAtt(files=train_files, load_path=load_path, T_length=3, all_label=True, CH=CH)\n",
    "valid_dataset = SpectrogramDatasetAtt(files=val_files, load_path=load_path, T_length=3, all_label=True, CH=CH)\n",
    "test_dataset = SpectrogramDatasetAtt(files=test_files, load_path=load_path, T_length=3, all_label=True, CH=CH)\n",
    "\n",
    "# train_dataset = SpectrogramDatasetAtt(files=train_files, load_path=load_path, T_length=3, all_label=False, CH=CH)\n",
    "# valid_dataset = SpectrogramDatasetAtt(files=val_files, load_path=load_path, T_length=3, all_label=False, CH=CH)\n",
    "# test_dataset = SpectrogramDatasetAtt(files=test_files, load_path=load_path, T_length=3, all_label=False, CH=CH)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle = True)\n",
    "val_loader = DataLoader(dataset=valid_dataset, batch_size=128, shuffle = False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 100, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for specs, labels, dates, recs, times in train_loader:\n",
    "    break\n",
    "specs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if input makes sense\n",
    "plt.imshow(specs[0][0, 0, :, :])\n",
    "plt.yticks(ticks=[0, 20, 40, 60, 80, 99], labels=[round(np.logspace(0, 2.45, 100)[i]) for i in [0, 20, 40, 60, 80, 99]])\n",
    "plt.xticks(ticks=[0, 9], labels=[1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding_init(n_position, emb_dim):\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / emb_dim) for j in range(emb_dim)]\n",
    "        if pos != 0 else np.zeros(emb_dim) for pos in range(n_position)])\n",
    "    \n",
    "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # apply sin on 0th,2nd,4th...emb_dim\n",
    "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # apply cos on 1st,3rd,5th...emb_dim\n",
    "    return torch.from_numpy(position_enc).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = position_encoding_init(10, 30).numpy().T\n",
    "plt.imshow(a)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN and Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "# https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, p_dropout, out_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # CNN\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=4, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.bn1 = nn.BatchNorm2d(2)\n",
    "        self.bn2 = nn.BatchNorm2d(4)\n",
    "        self.fc1 = nn.Linear(4*25*2, out_dim)\n",
    "              \n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = x.float()\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.shape[0],-1) \n",
    "        x = self.dropout(x) # perhaps remove dropout\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class Attention1(nn.Module):\n",
    "    def __init__(self, att_dim, T_length, p_dropout, all_label=False):\n",
    "        super(Attention1, self).__init__()\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.position_enc = nn.Embedding(T_length, att_dim)\n",
    "        self.position_enc.weight.data = position_encoding_init(T_length, att_dim)\n",
    "\n",
    "        # Attention\n",
    "        self.query = nn.Linear(att_dim, att_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=-1) # every slice along dim will sum to 1\n",
    "        self.ff1 = nn.Linear(att_dim, att_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ff2 = nn.Linear(att_dim, att_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(att_dim, 1) # paper uses 2 layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "       \n",
    "        self.T_length = T_length\n",
    "        self.all_label = all_label\n",
    "        \n",
    "    def forward(self, x, x_pos):\n",
    "        x = x.float()\n",
    "            \n",
    "#         x += self.position_enc(x_pos)\n",
    "        Q = self.query(x)  # Q = X*W + B\n",
    "        Q = self.dropout(Q)\n",
    "        \n",
    "        energy =  torch.bmm(Q, x.permute(0, 2, 1)) # Q*X_T\n",
    "        attention = self.softmax(self.tanh(energy)) # alpha\n",
    "#         attention = self.softmax(energy)\n",
    "        x_att = torch.bmm(attention, x) # Attention \n",
    "\n",
    "        x_att = self.dropout(x_att)\n",
    "#         x_output = torch.sigmoid(self.fc(x_att))\n",
    "        x_fc = self.ff2(self.relu(self.ff1(x_att)))\n",
    "        x_fc = self.dropout(x_fc)\n",
    "        x_output = torch.sigmoid(self.fc(x_fc))\n",
    "      \n",
    "        if self.all_label:\n",
    "            return x_output\n",
    "        return x_output[:, int((self.T_length-1)/2), :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ScaledDotProductAttention(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Scaled Dot-Product Attention proposed in \"Attention Is All You Need\"\n",
    "#     Compute the dot products of the query with all keys, divide each by sqrt(dim),\n",
    "#     and apply a softmax function to obtain the weights on the values\n",
    "#     Args: dim, mask\n",
    "#         dim (int): dimention of attention\n",
    "#         mask (torch.Tensor): tensor containing indices to be masked\n",
    "#     Inputs: query, key, value, mask\n",
    "#         - **query** (batch, q_len, d_model): tensor containing projection vector for decoder.\n",
    "#         - **key** (batch, k_len, d_model): tensor containing projection vector for encoder.\n",
    "#         - **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n",
    "#         - **mask** (-): tensor containing indices to be masked\n",
    "#     Returns: context, attn\n",
    "#         - **context**: tensor containing the context vector from attention mechanism.\n",
    "#         - **attn**: tensor containing the attention (alignment) from the encoder outputs.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, dim: int):\n",
    "#         super(ScaledDotProductAttention, self).__init__()\n",
    "#         self.sqrt_dim = np.sqrt(dim)\n",
    "\n",
    "#     def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "#         score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim\n",
    "\n",
    "#         if mask is not None:\n",
    "#             score.masked_fill_(mask.view(score.size()), -float('Inf'))\n",
    "\n",
    "#         attn = F.softmax(score, -1)\n",
    "#         context = torch.bmm(attn, value)\n",
    "#         return context, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify models and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.001\n",
    "# num_epochs = 31\n",
    "# model_type = 'CNN-Attention'\n",
    "# loss_type = 'bce'\n",
    "# p_dropout = 0.2\n",
    "# p_dropout_att = 0.2\n",
    "# att_dim = 128\n",
    "# T_length = 3\n",
    "# all_label = True\n",
    "# verbose = True\n",
    "\n",
    "# model_CNN = CNN(p_dropout, att_dim).to(device)\n",
    "# model_Att = Attention1(att_dim, T_length,p_dropout_att, all_label).to(device)\n",
    "\n",
    "\n",
    "# specs, labels, dates, recs, times = next(iter(train_loader))\n",
    "# labels = torch.stack(labels).transpose(1,0)\n",
    "# labels = labels.to(device).float()\n",
    "\n",
    "# torch.LongTensor([list(range(T_length))])\n",
    "# CNN_outputs = torch.zeros(specs[0].shape[0], len(specs), att_dim, device=device) \n",
    "\n",
    "    \n",
    "# for t in range(T_length):\n",
    "#     data_t = specs[t].to(device).float()\n",
    "#     CNN_outputs[:, t, :] = model_CNN(data_t)\n",
    "#     print(CNN_outputs.shape)\n",
    "# pos = torch.LongTensor([list(range(T_length))]).to(device)\n",
    "# Att_outputs = model_Att(CNN_outputs, pos)\n",
    "# print(Att_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = (Att_outputs > 0.5) * 1.0\n",
    "# len(predictions)\n",
    "# predictions = predictions.flatten().detach().cpu().numpy()\n",
    "# labels = labels.flatten().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model_CNN, model_Att, loader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels, _, _, _ in loader:\n",
    "            labels = torch.stack(labels).transpose(1,0)\n",
    "            labels = labels.to(device).float()\n",
    "            CNN_outputs = torch.zeros(data[0].shape[0], len(data), att_dim, device=device) \n",
    "            \n",
    "            # for all the spectrogram in the group, stack up the CNN output\n",
    "            for t in range(T_length):\n",
    "                data_t = data[t].to(device).float()\n",
    "                CNN_outputs[:, t, :] = model_CNN(data_t) # batch size x attention dim\n",
    "            pos = torch.LongTensor([list(range(T_length))]).to(device)\n",
    "            Att_outputs = model_Att(CNN_outputs, pos)\n",
    "            predictions = (Att_outputs > 0.5) * 1.0\n",
    "            predictions = predictions.flatten().detach().cpu().numpy()\n",
    "            labels = labels.flatten().cpu().numpy()\n",
    "            total += len(labels)\n",
    "            correct += (predictions == labels).sum()\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_CNN, model_Att, optimizer_CNN, optimizer_Att, criterion, loader, T_length, device):\n",
    "    model_CNN.train()\n",
    "    model_Att.train()\n",
    "    epoch_losses = 0\n",
    "    epoch_lens = 0\n",
    "    \n",
    "    for batch_idx, (data, labels, _, _, _) in enumerate(loader):\n",
    "        labels = torch.stack(labels).transpose(1,0)\n",
    "        labels = labels.to(device).float()\n",
    "        CNN_outputs = torch.zeros(data[0].shape[0], len(data), att_dim, device=device)\n",
    "        for t in range(T_length):\n",
    "            data_t = data[t].to(device).float()\n",
    "            CNN_outputs[:, t, :] = model_CNN(data_t)\n",
    "        pos = torch.LongTensor([list(range(T_length))]).to(device)\n",
    "        Att_outputs = model_Att(CNN_outputs, pos)\n",
    "        loss = criterion(Att_outputs, labels)\n",
    "        epoch_losses += loss\n",
    "        epoch_lens += 1\n",
    "\n",
    "        optimizer_CNN.zero_grad()\n",
    "        optimizer_Att.zero_grad()\n",
    "        loss.backward() # Will back propogation work correctly?\n",
    "        optimizer_CNN.step()\n",
    "        optimizer_Att.step()\n",
    "    \n",
    "    epoch_accs = get_accuracy(model_CNN, model_Att, loader, device=device)\n",
    "    return epoch_losses/epoch_lens, epoch_accs\n",
    "\n",
    "def evaluate(model_CNN, model_Att, optimizer_CNN, optimizer_Att, criterion, loader, T_length, device='cuda'):\n",
    "    \n",
    "    model_CNN.eval()\n",
    "    model_Att.eval()\n",
    "    epoch_losses = 0\n",
    "    epoch_lens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels, _, _, _) in enumerate(loader):\n",
    "            labels = torch.stack(labels).transpose(1,0)\n",
    "            labels = labels.to(device).float()\n",
    "            CNN_outputs = torch.zeros(data[0].shape[0], len(data), att_dim, device=device)\n",
    "            for t in range(T_length):\n",
    "                data_t = data[t].to(device).float()\n",
    "                CNN_outputs[:, t, :] = model_CNN(data_t)\n",
    "            pos = torch.LongTensor([list(range(T_length))]).to(device)\n",
    "            Att_outputs = model_Att(CNN_outputs, pos)\n",
    "            loss = criterion(Att_outputs, labels)\n",
    "            epoch_losses += loss\n",
    "            epoch_lens += 1\n",
    "    \n",
    "    epoch_accs = get_accuracy(model_CNN, model_Att, loader, device=device)\n",
    "    return epoch_losses/epoch_lens, epoch_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify models and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 31\n",
    "model_type = 'CNN-Attention'\n",
    "loss_type = 'bce'\n",
    "p_dropout = 0.2\n",
    "p_dropout_att = 0.2\n",
    "att_dim = 128\n",
    "T_length = 3\n",
    "all_label = True\n",
    "verbose = True\n",
    "\n",
    "model_CNN = CNN(p_dropout, att_dim).to(device)\n",
    "model_Att = Attention1(att_dim, T_length, p_dropout_att, all_label).to(device)\n",
    "\n",
    "optimizer_CNN = torch.optim.Adam(model_CNN.parameters(), lr = learning_rate)\n",
    "optimizer_Att = torch.optim.Adam(model_Att.parameters(), lr = learning_rate)\n",
    "criterion = nn.BCELoss() # BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention1(\n",
       "  (position_enc): Embedding(3, 128)\n",
       "  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (ff1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (ff2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "torch.manual_seed(90)\n",
    "model_CNN.apply(weights_init)\n",
    "model_Att.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention1 (separate dates, upsample only train, all labels for Attention1)\n",
    "train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model_CNN, model_Att, optimizer_CNN, optimizer_Att, criterion, train_loader, T_length=3, device='cuda')\n",
    "    val_loss, val_acc = evaluate(model_CNN, model_Att, optimizer_CNN, optimizer_Att, criterion, val_loader, T_length=3, device='cuda')\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    if val_loss <= min(val_losses):\n",
    "        best_epoch = epoch\n",
    "        print(epoch)\n",
    "        print('Train loss for epoch {}: {}'.format(epoch, train_loss))\n",
    "        print('Val loss for epoch {}: {}'.format(epoch, val_loss))\n",
    "        # torch.save(model_CNN.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "        # torch.save(model_Att.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "    elif verbose:\n",
    "        print('Train loss for epoch {}: {}'.format(epoch, train_loss))\n",
    "        print('Val loss for epoch {}: {}'.format(epoch, val_loss))\n",
    "        \n",
    "    # if epoch == num_epochs-1:\n",
    "        # torch.save(model.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "        # torch.save(model_CNN.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "\n",
    "plot_loss_acc(train_losses, val_losses, train_accs, val_accs, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(train_losses, val_losses, train_accs, val_accs, model_type)\n",
    "plt.savefig(load_path + 'figures/Goose_1st/'+'CNN_Attention_with_pos_enc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 51\n",
    "model_type = 'CNN-Attention'\n",
    "loss_type = 'bce'\n",
    "p_dropout = 0.2\n",
    "att_dim = 128\n",
    "T_length = 3\n",
    "all_label = False\n",
    "verbose = True\n",
    "\n",
    "model_CNN = CNN(p_dropout, att_dim).to(device)\n",
    "model_Att = Attention1(att_dim, T_length, all_label).to(device)\n",
    "optimizer_CNN = torch.optim.Adam(model_CNN.parameters(), lr = learning_rate)\n",
    "optimizer_Att = torch.optim.Adam(model_Att.parameters(), lr = learning_rate)\n",
    "criterion = nn.BCELoss() # BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention1 (separate dates, upsample only train, mid labels for Attention1)\n",
    "train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model_CNN, model_Att, optimizer_CNN, optimizer_Att, criterion, train_loader, T_length=3, device='cuda')\n",
    "    val_loss, val_acc = evaluate(model_CNN, model_Att, optimizer_CNN, optimizer_Att, criterion, val_loader, T_length=3, device='cuda')\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    if val_loss <= min(val_losses):\n",
    "        best_epoch = epoch\n",
    "        print(epoch)\n",
    "        print('Train loss for epoch {}: {}'.format(epoch, train_loss))\n",
    "        print('Val loss for epoch {}: {}'.format(epoch, val_loss))\n",
    "        # torch.save(model_CNN.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "        # torch.save(model_Att.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "    elif verbose:\n",
    "        print('Train loss for epoch {}: {}'.format(epoch, train_loss))\n",
    "        print('Val loss for epoch {}: {}'.format(epoch, val_loss))\n",
    "        \n",
    "    # if epoch == num_epochs-1:\n",
    "        # torch.save(model.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "        # torch.save(model_CNN.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "\n",
    "plot_loss_acc(train_losses, val_losses, train_accs, val_accs, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 51\n",
    "model_type = 'CNN-Attention'\n",
    "loss_type = 'bce'\n",
    "p_dropout = 0.2\n",
    "att_dim = 128\n",
    "T_length = 3\n",
    "all_label = True\n",
    "verbose = True\n",
    "\n",
    "model_CNN = CNN(p_dropout, att_dim).to(device)\n",
    "model_Att = Attention1(att_dim, T_length, all_label).to(device)\n",
    "optimizer_CNN = torch.optim.Adam(model_CNN.parameters(), lr = learning_rate)\n",
    "optimizer_Att = torch.optim.Adam(model_Att.parameters(), lr = learning_rate)\n",
    "criterion = nn.BCELoss() # BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention1 (separate dates, upsample train/val, all labels for Attention1)\n",
    "train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model_CNN, model_Att, optimizer_CNN, optimizer_Att, criterion, train_loader, T_length=3, device='cuda')\n",
    "    val_loss, val_acc = evaluate(model_CNN, model_Att, optimizer_CNN, optimizer_Att, criterion, val_loader, T_length=3, device='cuda')\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    if val_loss <= min(val_losses):\n",
    "        best_epoch = epoch\n",
    "        print(epoch)\n",
    "        print('Train loss for epoch {}: {}'.format(epoch, train_loss))\n",
    "        print('Val loss for epoch {}: {}'.format(epoch, val_loss))\n",
    "        # torch.save(model_CNN.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "        # torch.save(model_Att.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "    elif verbose:\n",
    "        print('Train loss for epoch {}: {}'.format(epoch, train_loss))\n",
    "        print('Val loss for epoch {}: {}'.format(epoch, val_loss))\n",
    "        \n",
    "    # if epoch == num_epochs-1:\n",
    "        # torch.save(model.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "        # torch.save(model_CNN.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "\n",
    "plot_loss_acc(train_losses, val_losses, train_accs, val_accs, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 51\n",
    "model_type = 'CNN-Attention'\n",
    "loss_type = 'bce'\n",
    "p_dropout = 0.2\n",
    "att_dim = 128\n",
    "T_length = 3\n",
    "all_label = True\n",
    "verbose = True\n",
    "\n",
    "model_CNN = CNN(p_dropout, att_dim).to(device)\n",
    "model_Att = Attention1(att_dim, T_length, all_label).to(device)\n",
    "optimizer_CNN = torch.optim.Adam(model_CNN.parameters(), lr = learning_rate)\n",
    "optimizer_Att = torch.optim.Adam(model_Att.parameters(), lr = learning_rate)\n",
    "criterion = nn.BCELoss() # BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention1 (separate dates, upsample train/val, all labels for Attention1, not shuffle val/test)\n",
    "train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model_CNN, model_Att, optimizer_CNN, optimizer_Att, criterion, train_loader, T_length=3, device='cuda')\n",
    "    val_loss, val_acc = evaluate(model_CNN, model_Att, optimizer_CNN, optimizer_Att, criterion, val_loader, T_length=3, device='cuda')\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    if val_loss <= min(val_losses):\n",
    "        best_epoch = epoch\n",
    "        print(epoch)\n",
    "        print('Train loss for epoch {}: {}'.format(epoch, train_loss))\n",
    "        print('Val loss for epoch {}: {}'.format(epoch, val_loss))\n",
    "        # torch.save(model_CNN.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "        # torch.save(model_Att.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "    elif verbose:\n",
    "        print('Train loss for epoch {}: {}'.format(epoch, train_loss))\n",
    "        print('Val loss for epoch {}: {}'.format(epoch, val_loss))\n",
    "        \n",
    "    # if epoch == num_epochs-1:\n",
    "        # torch.save(model.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "        # torch.save(model_CNN.state_dict(), '{}/{}_CH{}_LOSS{}_TW{}_TLEN{}_DROP{}_ATTDIM{}_EPOCH{}.pt'.format(path, model_type, CH, loss_type, time_window, T_length, p_dropout, att_dim, epoch))\n",
    "\n",
    "plot_loss_acc(train_losses, val_losses, train_accs, val_accs, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = []\n",
    "for a,b,c,d,e in val_loader:\n",
    "    val_labels.extend(b.numpy().flatten())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN.eval()\n",
    "model_Att.eval()\n",
    "epoch_losses = 0\n",
    "val_preds, val_labels = [], []    \n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, labels, _, _, _) in enumerate(val_loader):\n",
    "        labels = labels.to(device).float()\n",
    "        CNN_outputs = torch.zeros(data[0].shape[0], len(data), att_dim, device=device)\n",
    "        for t in range(T_length):\n",
    "            data_t = data[t].to(device).float()\n",
    "            CNN_outputs[:, t, :] = model_CNN(data_t)\n",
    "        pos = torch.LongTensor([list(range(T_length))]).to(device)\n",
    "        Att_outputs = model_Att(CNN_outputs, pos)\n",
    "        val_preds.extend(Att_outputs.detach().cpu().numpy().flatten())\n",
    "        val_labels.extend(labels.detach().cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (np.array(val_preds) > 0.5) * 1.0\n",
    "b = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(range(len(b)), b, s=0.5)\n",
    "plt.title('Labels')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(range(len(a)), a, s=0.5)\n",
    "plt.title('Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
